Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 朴素的想法

从过拟合的现象分析，是因为神经网络的权重矩阵参数过度地学习，即针对训练集，其损失函数值已经逼近了最小值。我们用熟悉的等高线图来解释：

<img src=".\Images\16\L1.png">

假设只有两个参数需要学习，那么这两个参数的损失函数就构成了上面的等高线图。由于样本数据量比较小（这是造成过拟合的原因之一），所以神经网络在训练过程中沿着箭头方向不断向最优解靠近，最终达到了过拟合的状态。也就是说在这个等高线图中的最优解，实际是针对有限的样本数据的最优解，而不是针对这个特点问题的最优解。

由此会产生一个朴素的想法：如果我们以某个处于中间位置等高线上（比如那条红色的等高线）为目标的话，是不是就可以得到比较好的效果呢？这条登高线如何找到呢？

# 基本数学知识

## 范数

回忆一下范数的基本概念：

$$L_p = \lVert x \rVert_p = ({\sum^n_{i=1}\lvert x_i \rvert^p})^{1/p} \tag{1}$$

范数包含向量范数和矩阵范数，我们只关心向量范数。我们用具体的数值来理解范数。假设有一个向量a：

$$a=[1,-2,0,-4]$$

$$L_0=3 \tag{非0元素数}$$
$$L_1 = \sum^3_{i=0}\lvert x_i \rvert = 1+2+0+4=7 \tag{绝对值求和}$$
$$L_2 = \sqrt[2]{\sum^3_{i=0}\lvert x_i \rvert^2} =\sqrt[2]{21}=4.5826 \tag{平方和求方根}$$
$$L_{\infty}=4 \tag{最大值的绝对值}$$

注意p可以是小数，比如0.5：

$$L_{0.5}=19.7052$$

一个经典的关于P范数的变化图如下：

<img src=".\Images\16\norm.png">

我们只关心L1和L2范数：
- L1范数是个菱形体，在平面上是一个菱形
- L2范数是个球体，在平面上是一个圆

## 高斯分布

$$
f(x)={1 \over \sigma\sqrt{2 \pi}} exp{- {(x-\mu)^2} \over 2\sigma^2} \tag{2}
$$

请参考15.2一节。

# L2正则化

假设：
- W参数服从高斯分布，即：$w_j \sim N(0,\tau^2)$
- Y服从高斯分布，即：$y_i \sim N(w^Tx_i,\sigma^2)$

贝叶斯最大后验估计：

$$
argmax_wL(w) = ln \prod_i^n {1 \over \sigma\sqrt{2 \pi}}exp(-(\frac{y_i-w^Tx_i}{\sigma})^2/2) \cdot \prod_j^m{\frac{1}{\tau\sqrt{2\pi}}exp(-(\frac{w_j}{\tau})^2/2)}
$$

$$
=-\frac{1}{2\sigma^2}\sum_i^n(y_i-w^Tx_i)-\frac{1}{2\tau^2}\sum_j^m{w_j^2}-n\ln\sigma\sqrt{2\pi}-m\ln \tau\sqrt{2\pi} \tag{3}
$$
则函数f(w)的最小值为：
$$
argmin_wf(w) = \sum_i^n(y_i-w^Tx_i)^2+\lambda\sum_j^m{w_j^2} \tag{4}
$$

看公式4，相当于是线性回归的均方差损失函数，再加上一个正则项（称为惩罚项），共同构成损失函数。如果想求这个函数的最小值，则需要两者协调，并不是说分别求其最小值实现整体最小，因为它们具有共同的W项，当W比较大时，第一项比较小，第二项比较大，或者正好相反。所以它们是矛盾组合体。

对于第一项，我们用前面学习过损失函数的等高线图来解释。对于第二项，如果W是一个含有两个参数的矢量，则第二项的形式应该是一个圆形。所以，结合两者，我们可以得到这样一张图：

<img src=".\Images\16\L2.png">

黄色的圆形，就是正则项所处的区域。这个区域的大小，是由参数$\lambda$所控制的，该值越大，黄色圆形区域越大。比如图中分别标出了该值为0.7、0.8、0.9的情况。

Ridge Regression岭回归
Weight Decay权值衰减


假设是均方差损失函数：

$$J(w,b)=\frac{1}{2m}[\sum_{i=1}^m (a_i-y_i)^2 + \lambda\sum_{j=1}^n{w_j^2}]$$


如果是交叉熵损失函数：

$$J(w,b)= -\frac{1}{m} \sum_{i=1}^m [y_i \log a_i + (1-y_i) \log (1-a_i)]+ \frac{\lambda}{2m}\sum_{j=1}^n{w_j^2}$$

由于正则项是在损失函数中，在正向计算中，并不涉及到它，所以正向计算公式不用变。但是在反向传播过程中，需要重新推导一下公式。

假设有一个两层的回归神经网络，其前向计算如下：

$$
Z1 = W1 \cdot X + B1 \tag{5}
$$
$$
A1 = Sigmoid(Z1) \tag{6}
$$
$$
Z2 = W2 \cdot A1 + B2 \tag{7}
$$
$$
J(w,b)=\frac{1}{2m}[\sum_{i=1}^m (z_i-y_i)^2 + \lambda\sum_{j=1}^n{w_j^2}]  \tag{8}
$$
从公式8求Z2的误差矩阵：
$$
\frac{dJ}{dZ2}=Z2-Y
$$
从公式8求W2的误差矩阵，因为有正则项存在，所以需要附加一项：
$$
\frac{dJ}{dW2}=\frac{dJ}{dZ2}\frac{dZ2}{dW2}+\frac{dJ}{dW2}
$$
$$
=(Z2-Y)\cdot A1^T+\lambda \odot W2
$$


参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。 正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。 正则化可以保持模型简单，另外，规则项的使用还可以约束我们的模型的特性。


L2范数

除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减”(weight decay)。 weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。

L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

## L1&L2 Norm

正则化项一般使用在损失函数的计算中，用于约束某些权重不至于过大，导致网络爆炸或者过拟合现象的产生，一般使用的形式为：
$$ loss = J(predict, groundtruth) + \alpha Regulazation$$
这里的正则化即代表了对参数的约束条件，$\alpha$参数代表了在当前正则化项中对参数的约束程度。目前常常使用的正则化方法是L1正则化和L2正则化，下面我们将分别介绍这两种方法

### L1 Norm

L1正则化项，代表了参数的曼哈顿距离，其实就是将各个参数的绝对值进行相加，约束这样一个参数和不至于过大，

$$ L_1 = \sum_i|w_i| = ||w||_1$$

使用L1正则化去训练模型有助于让我们得到一个相对稀疏的权重矩阵，以二维空间为例，我们做出$|w_1| + |w_2| = k$这样的矩阵如下图：

<img src=".\Images\16\3.png">


我们在这样一个矩形框上进行均匀采样的话，会发现所采集的样本会比较集中在四个角上，也就是说，会有更多的权重会趋近于零，也就造成了权重矩阵的稀疏性。

这样一个L1 Norm的前向和反向传播是很显然的，也就是根据参数的符号，传递1和-1即可。也就是说，假设原始$w_i$对应的梯度是$d$的话，那么我们得到的新的梯度就是

$$d^/' = d + \alpha \times sign(w_i)$$

### L2 Norm

比起L1 Norm， L2 Norm的使用更为频繁，L2 Norm就是欧氏距离，采用的是约束的参数的平方和的形式，

$$ L_2 = \sum_i (w_i)^2 = ||w||_2$$

$$ loss = J(predict, groundtruth) + \lambda \frac{1}{2m} \sum_i^m (w_i)^2$$

使用L2 Norm能够约束各个参数不至于过大，同时也不会造成很大的稀疏性，因为L2 Norm形成的边框是一个圆，在这个边框上进行采样并不会有大量的数据集中在坐标轴附近，而是较为均匀的分布的。

<img src=".\Images\16\4.png">

类似得，假设原始$w_i$的梯度是$d$的话，我们可以得到加上L2 Norm之后的梯度$d^/'$为：

$$d^/' = d + \frac{\lambda}{m}w_i$$

# 参考资料

http://charleshm.github.io/2016/03/Regularized-Regression/

https://blog.csdn.net/red_stone1/article/details/80755144

https://www.jianshu.com/p/c9bb6f89cfcc
