Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 丢弃法 Dropout

2012年，Alex、Hinton在其论文《ImageNet Classification with Deep Convolutional Neural Networks》中用到了Dropout算法，用于防止过拟合。并且，这篇论文提到的AlexNet网络模型引爆了神经网络应用热潮，并赢得了2012年图像识别大赛冠军，使得CNN成为图像分类上的核心算法模型。

随后，又有一些关于Dropout的文章《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》、《Improving Neural Networks with Dropout》、《Dropout as data augmentation》。

从上面的论文中，我们能感受到Dropout在深度学习中的重要性。那么，到底什么是Dropout呢？

我们假设原来的神经网络是这个结构，最后输出三分类结果：

<img src=".\Images\16\dropout1.png">

Dropout可以作为训练深度神经网络的一种正则方法供选择。在每个训练批次中，通过忽略一部分的神经元（让其隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少隐层节点间的相互作用，高层的神经元需要低层的神经元的输出才能发挥作用。

Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的输出以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如下图所示：

<img src=".\Images\16\dropout2.png">

其中有叉子的神经元在本轮训练中被暂时的封闭了，在下一轮训练中，可能会随机地封闭其它一些神经元。

DropOut的作用是在训练过程中随机舍弃掉一定比例的节点，使用剩余的节点进行训练。采用这种方法可以让不同神经元节点接收到的训练数据不同，可以有效地防止神经元因为接收到过多的同类型参数而陷入过拟合的状态。


# 正向计算

# 反向传播




# 训练和测试阶段的不同

<img src=".\Images\16\dropout3.png">





### 算法思路

   1. 给定舍弃的比例$q$
   2. 按照这个比例选出一定的神经元将其输出置为0，也就相当于从网络中删掉这一些被选出来的神经元
   3. 为了保持各层输出的值大致相等，将剩余的神经元的输出方法$\frac{1}{q}$倍
   4. 继续进行传播和训练
   5. 在测试过程中将dropOut设为无效即可




关于Dropout，文章中没有给出任何数学解释，Hintion的直观解释和理由如下：
1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。
2. 可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同。
3. native bayes是dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。
4. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。

# 参考

- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
- http://jmlr.org/papers/v15/srivastava14a.html
- https://blog.csdn.net/program_developer/article/details/80737724

