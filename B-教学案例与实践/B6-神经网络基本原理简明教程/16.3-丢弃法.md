Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 丢弃法 Dropout

2012年，Alex、Hinton在其论文《ImageNet Classification with Deep Convolutional Neural Networks》中用到了Dropout算法，用于防止过拟合。并且，这篇论文提到的AlexNet网络模型引爆了神经网络应用热潮，并赢得了2012年图像识别大赛冠军，使得CNN成为图像分类上的核心算法模型。

随后，又有一些关于Dropout的文章《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》、《Improving Neural Networks with Dropout》、《Dropout as data augmentation》。

从上面的论文中，我们能感受到Dropout在深度学习中的重要性。那么，到底什么是Dropout呢？

我们假设原来的神经网络是这个结构，最后输出三分类结果：

<img src=".\Images\16\dropout1.png">

Dropout可以作为训练深度神经网络的一种正则方法供选择。在每个训练批次中，通过忽略一部分的神经元（让其隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少隐层节点间的相互作用，高层的神经元需要低层的神经元的输出才能发挥作用。

Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的输出以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如下图所示：

<img src=".\Images\16\dropout2.png">

其中有叉子的神经元在本轮训练中被暂时的封闭了，在下一轮训练中，可能会随机地封闭其它一些神经元。

DropOut的作用是在训练过程中随机舍弃掉一定比例的节点，使用剩余的节点进行训练。采用这种方法可以让不同神经元节点接收到的训练数据不同，可以有效地防止神经元因为接收到过多的同类型参数而陷入过拟合的状态。


# 正向计算

正常的隐层计算公式是：

$$
Z = W \cdot X + B \tag{1}
$$

加入随机丢弃步骤后，变成了：

$$
r \sim Bernoulli(p) \tag{2}
$$
$$Y = r * X \tag{3}$$
$$Z = W \cdot Y + B \tag{4}
$$

公式2是得到一个分布概率为p的伯努利分布，伯努利分布在这里可以简单地理解为0、1分布，p=0.5时，会生产与X相同数量的0、1，假设一共10个数，则：
$$
r=[0,0,1,1,0,1,0,1,1,0]
$$
或者
$$r=[0,1,1,0,0,1,0,1,0,1]
$$
或者其它一些分布。

从公式3，Y将会是X经过r的mask的结果，1的位置保留原x值，0的位置相乘后为0。

# 反向传播

在反向传播时，和Relu函数的反向差不多，需要记住正向计算时得到的r值，反向的误差矩阵直接乘以这个r值就可以了。

# 训练和测试阶段的不同

在训练阶段，我们使用正向计算的逻辑。在测试时，不能随机丢弃一些神经元，否则会造成测试结果不稳定，比如某个样本的第一次测试，得到了结果A；第二次测试，得到结果B。由于丢弃的神经元的不同，A和B肯定不相同，就会造成无法解释的情况。

但是如何模拟那些在训练时丢弃的神经元呢？我们任然可以利用训练时的概率，如下图所示：

<img src=".\Images\16\dropout3.png">

左图为训练时，输入的信号会以概率p存在，如果p=0.6，则会有40%的概率被丢弃，60%的概率存在。

右图为测试/推理时，输入信号总会存在，但是在每个输出上，都应该用原始的权重值，乘以概率p。

# 代码实现

```Python
class DropoutLayer(CLayer):
    def __init__(self, input_size, ratio=0.5):
        self.dropout_ratio = ratio
        self.mask = None
        self.input_size = input_size
        self.output_size = input_size

    def forward(self, input, train=True):
        assert(input.ndim == 2)
        if train:
            self.mask = np.random.rand(*input.shape) > self.dropout_ratio
            self.z = input * self.mask
        else:
            self.z = input * (1.0 - self.dropout_ratio)

        return self.z
       
    def backward(self, delta_in, idx):
        delta_out = self.mask * delta_in
        return delta_out
```

上面的代码中，ratio是丢弃率，如果ratio=0.4，则前面的原理解释中的p=1-0.4=0.6。

另外，我们可以看到，这里的DropoutLayer是作为一个层出现的，而不是寄生在全连接层内部。貌似位置不对，但其实把它看作全连接层之前的处理层就可以了，逻辑上是一样的。

写好Dropout层后，我们在原来的基础上，搭建一个带Dropout层的新网络，如下图：

<img src=".\Images\16\dropout5.png">

与以前的过拟合的网络相比，只是在每个层之间增加一个Drouput层。用代码理解的话，请看下面的函数：

```
def DropoutNet(dataReader, num_input, num_hidden1, num_hidden2, num_hidden3, num_hidden4, num_output, params):
    net = NeuralNet(params)

    fc1 = FcLayer(num_input, num_hidden1, params)
    net.add_layer(fc1, "fc1")
    relu1 = ActivatorLayer(Relu())
    net.add_layer(relu1, "relu1")
    
    drop1 = DropoutLayer(num_hidden1, 0.1)
    net.add_layer(drop1, "dp1")
    
    fc2 = FcLayer(num_hidden1, num_hidden2, params)
    net.add_layer(fc2, "fc2")
    relu2 = ActivatorLayer(Relu())
    net.add_layer(relu2, "relu2")
    
    drop2 = DropoutLayer(num_hidden2, 0.5)
    net.add_layer(drop2, "dp2")
    
    fc3 = FcLayer(num_hidden2, num_hidden3, params)
    net.add_layer(fc3, "fc3")
    relu3 = ActivatorLayer(Relu())
    net.add_layer(relu3, "relu3")
    
    drop3 = DropoutLayer(num_hidden3, 0.5)
    net.add_layer(drop1, "dp3")
    
    fc4 = FcLayer(num_hidden3, num_hidden4, params)
    net.add_layer(fc4, "fc4")
    relu4 = ActivatorLayer(Relu())
    net.add_layer(relu4, "relu4")
    
    drop4 = DropoutLayer(num_hidden4, 0.5)
    net.add_layer(drop4, "dp4")
    

    fc5 = FcLayer(num_hidden4, num_output, params)
    net.add_layer(fc5, "fc5")
    softmax = ActivatorLayer(Softmax())
    net.add_layer(softmax, "softmax")

    net.train(dataReader, checkpoint=5)
    
    net.ShowLossHistory()
```

运行程序，最后可以得到这样的损失函数图和验证结果：

<img src=".\Images\16\dropout4.png">

在损失函数值方面，改善并不大。但是在验证集的准确度验证上，可以达到90%的准确度，而过拟合网络只能得到最高82%的准确度。说明我们的Dropout层确实起到了作用。


# 直观理解

关于Dropout，论文中没有给出任何数学解释，Hintion的直观解释和理由如下：
1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。
2. 可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。
3. native bayes是dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。
4. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。由于性别是一半一半的比例，所以Dropout中的p一般设置为0.5。

# 参考

- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
- http://jmlr.org/papers/v15/srivastava14a.html
- https://blog.csdn.net/program_developer/article/details/80737724

